## Sampling rate
#fs: 16000
fs: 22050
#fs: 24000
#fs: 44100
#fs: 48000

## Frame-shift mel-cep/mel-spec extraction
shiftms: 5
#shiftms: 10

## Window length mel-spectrogram extraction
winms: 27.5

## DC-component removal
highpass_cutoff: 65

## Mel-cepstrum
#mcep_dim: 34
mcep_dim: 49

## Mel-spectrogram
mel_dim: 80

## Pre-emphasis noise-shaping coefficient
alpha: 0.85

## Coefficient for global variance (GV) postfilter
gv_coeff: 0.9

## GRU hidden units encoder, decoder spec, decoder excit
hidden_units_enc: 1024
#hidden_units_enc: 768
hidden_units_dec: 1024
#hidden_units_dec: 768
hidden_units_lf0: 1024
#hidden_units_lf0: 768
## set to 768 for allowing real-time/low-latency processing

## GRU hidden units wavernn (w/ dual-GRU)
hidden_units_wave: 384
#hidden_units_wave: 640
hidden_units_wave_2: 16
#hidden_units_wave_2: 32

## GRU hidden layers encoder, decoder spec, decoder excit
hidden_layers_enc: 1
hidden_layers_dec: 1
hidden_layers_lf0: 1

## GRU hidden layers wavernn
hidden_layers_wave: 1

## kernel-size input conv encoder
kernel_size_enc: 7
dilation_size_enc: 1

## kernel-size input conv decoder spec
kernel_size_dec: 7
dilation_size_dec: 1

## kernel-size input conv encoder excit
kernel_size_lf0: 7
dilation_size_lf0: 1

## kernel-size input conv wavernn
kernel_size_wave: 7
dilation_size_wave: 1

## shallow WaveNet hyperparams.
kernel_size: 7
hid_chn: 256
skip_chn: 256
dilation_depth: 3
dilation_repeat: 2

## for using skewed input convolution (> 0), or balanced (0) [only for encoder]
right_size: 0
## lookup frame limited to only 1 frame [for allowing low-latency/real-time processing]
#right_size: 1 

## number of cycles :  n_half_cyc // 2
#n_half_cyc: 1
n_half_cyc: 2
#n_half_cyc: 4
#n_half_cyc: 6
## if cyclevae, use n_half_cyc:2, i.e., 1 cycle
## if cyclevqvae, use n_half_cyc:4, i.e., 2 cycles

## spect. latent dim.
##cyclevae
lat_dim: 32 
##cyclevqvae spect est.
#lat_dim: 40 
##cyclevqvae spect+excit est.
#lat_dim: 50 
## use 32 for cyclevae
## use 40 for cyclevqvae spec, 50 for cyclevqvae spec-excit

## excit. latent dim.
##cyclevae
lat_dim_e: 32
##cyclevqvae spect+excit est.
#lat_dim_e: 50
## use 32 for cyclevae
## use 50 for cyclevqvae spec-excit

## codebook size for cyclevqvae
ctr_size: 128

## wave. quantization (mu-law)
n_quantize: 256

## use causal input convolution
## if using skewed input convolution for encoder (right_size > 0), set causal_conv dec/lf0 to true
## always use non-causal input conv. for encoder/wave
causal_conv_enc: false
causal_conv_dec: false
#causal_conv_dec: true
causal_conv_lf0: false
#causal_conv_lf0: true
causal_conv_wave: false
## if right_size > 0 (skewed conv encoder, i.e., future frame is limited), set causal_conv dec/lf0 to true [low-latency/real-time proc.]

## use bi-directional GRU
bi_enc: false
bi_dec: false
bi_lf0: false
bi_wave: false

## use AR feedback flow in encoder, decoder spec/excit(f0)
#ar_enc: true
ar_enc: false
ar_dec: true
#ar_dec: false
ar_f0: true
#ar_f0: false
## AR is not used in encoder both VAE and VQ-VAE
## if VAE, use AR in both decoders (spec & f0)
## if VQ-VAE, AR is not used in both decoders

## use differential features estimation in decoder spec (only for decoder with AR flow)
## if using VQ-VAE, differential features are not used
diff: true
#diff: false
## if VAE, use diff
## if VQ-VAE, diff is not used

## use excit input in cyclevae spec
f0in: true
#f0in: false
## only for cyclevae spec, for debugging, set f0in: true, cyclevqvae spec always true

## use detached graph of conv./cyclic feat
detach: true
#detach: false
## set true VAE & VQ-VAE

## use dim-reduction for cont. spk. embed from 1-hot code (only for cyclevae spec-excit)
#spkidtr_dim: 0
spkidtr_dim: 2
## only for cyclevae/cyclevqvae spec-excit, to freely interpolate between 2-dim cont. spk-embed spect & excit

## learning rate
lr: 1e-4

## dropout rate
do_prob: 0.5

## maximum epoch count
#epoch_count: 80
epoch_count: 90
#epoch_count: 100
#epoch_count: 120
## if using more lots of data, better to set to 120 to allow more training steps

## use get_max_frame.sh to get max_frame of the current train/dev sets as the value of pad_len
## zero pad length [in frames] for batch processing; for neural vocoder, still use in frame length, but in computation it is multiplied by number of samples per frame
##arctic [bdl] 5 ms shift
#pad_len: 1207
##vctk [p252] 5 ms shift
#pad_len: 1955
##vcc18 [VCC2SM4,20026] 5 ms shift
pad_len: 2149
##vcc20 [TMM1,M10036] 5 ms shift
#pad_len: 2215

## Batch setting for spectral/excitation modeling
## 30 frames for 5 ms shift
batch_size: 30
## 15 frames for 10 ms shift
#batch_size: 15
## cyclevae vcc20, rule of thumb is to let each epoch at least has 120 distinct batches, 60 train. utterances x 14 speakers in vcc20 = 840 / 6 = 140 distinct batches per epoch
#batch_size_utt: 6
## cyclevae vcc20+vctk(+vcc18+arctic), recommended maximum number of batch_size_utt is 55, regardless the number of training utterances goes beyond vcc20+vctk, i.e., 840 + 315 x 24 vctk spks = 8400
#batch_size_utt: 55
## cyclevae vcc18+arctic
#batch_size_utt: 20
## cyclevae vcc18/vcc20
batch_size_utt: 6
## batch_size eval just follow the number of utterances in the corresponding dev. set or any number is ok
batch_size_utt_eval: 10

## Batch setting for waveform modeling
## 30 frames x number of samples per frame, e.g., 30 x 120 for 24 kHz with 5 ms frame shift = 3600 batch smples
#batch_size_wave: 30
## 15 for 24 kHz with 10 ms shift --> 3600 samples
#batch_size_wave: 15
## 33 frames x number of samples per frame, e.g., 33 x 110 for 22.05 kHz with 5 ms frame shift = 3630 batch smples
batch_size_wave: 33
## 17 for 22.05 kHz with 10 ms shift --> 3740 samples
#batch_size_wave: 17
## 45 for 16 kHz wavenet with 5 ms shift --> 3600 samples; rule of thumb is for wavenet use batch_size = 3600, and batch_size_utt = 2, regardless sampling rate and frame shift
#batch_size_wave: 45
## 23 for 16 kHz with 10 ms shift --> 3680 samples
#batch_size_wave: 23
## 15 for wavernn/compact-wavernn with 5 ms frame shift regardless of sampling rate
#batch_size_wave: 15
## 8 for wavernn/compact-wavernn with 10 ms frame shift regardless of sampling rate
#batch_size_wave: 8
## wavenet, use batch_size (samples) equal to 3600 and batch_size_utt = 2, totaling 7200 samples in each optimization step
##          -> best value for wavenet modeling with more stability using 2 sequences per optim. step
batch_size_utt_wave: 2
## wavernn/compact-wavernn --> use batch_size_utt = 8
#batch_size_utt_wave: 8
## batch_size eval just follow the number of utterances in the corresponding dev. set or any number is ok
#batch_size_utt_eval_wave: 14
batch_size_utt_eval_wave: 16

n_workers: 2

#mdl_name: cycmcepvae-laplace
mdl_name: cycmceplf0capvae-laplace
#mdl_name: cycmcepvae-vq
#mdl_name: cycmceplf0capvae-vq

#mdl_name_wave: wavenet
mdl_name_wave: wavernn_dualgru_compact_lpc
#mdl_name_wave: wavernn_dualgru_compact_lpcseg

## number of data-driven linear predictive coefficients (LPC) in wavernn
lpc: 4

## sparsification scheduling for compact wavernn
t_start: 1551
#t_start: 20001
t_end: 364650
#t_end: 2342000
#interval: 50
interval: 100
densities: 0.05-0.05-0.2
#densities: 0.27-0.27-0.39
n_stage: 4

## number of multiple samples output for compact wavernn with segment output
seg: 2
#seg: 5

use_mcep: true
## mel-spectrogram-based modeling
#use_mcep: false
